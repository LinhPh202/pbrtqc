import streamlit as st
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

# =========================================================
# üöÄ PH·∫¶N 1: OPTIMIZED CACHING & DATA PROCESSING
# =========================================================

# D√πng decorator n√†y ƒë·ªÉ Streamlit nh·ªõ k·∫øt qu·∫£, kh√¥ng ph·∫£i t√≠nh l·∫°i m·ªói l·∫ßn
@st.cache_data(show_spinner=False)
def load_and_clean_data(file_train, file_verify, col_name):
    """ƒê·ªçc file v√† ti·ªÅn x·ª≠ l√Ω data (Cache l·∫°i)"""
    try:
        df_train = pd.read_excel(file_train)
        df_verify = pd.read_excel(file_verify)
        
        # L·∫•y d·ªØ li·ªáu d·∫°ng m·∫£ng numpy ngay l·∫≠p t·ª©c ƒë·ªÉ nhanh h∆°n
        data_train = df_train[col_name].dropna().values
        data_verify = df_verify[col_name].dropna().values
        
        return data_train, data_verify
    except Exception as e:
        return None, None

@st.cache_data(show_spinner=False)
def find_optimal_truncation(data, max_cut_percent=0.10, steps=10):
    """T√¨m kho·∫£ng c·∫Øt t·ªëi ∆∞u (ƒê√£ t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô)"""
    # Ch·ªâ l·∫•y m·∫´u t·ªëi ƒëa 5000 ƒëi·ªÉm ƒë·ªÉ t√≠nh Shapiro cho nhanh n·∫øu data qu√° l·ªõn
    # Data g·ªëc v·∫´n gi·ªØ nguy√™n, ch·ªâ d√πng sample ƒë·ªÉ t√¨m ng∆∞·ª°ng
    calc_data = data
    if len(data) > 5000:
        np.random.seed(42)
        calc_data = np.random.choice(data, 5000, replace=False)
        
    best_p = -1
    best_range = (data.min(), data.max())
    
    cuts = np.linspace(0, max_cut_percent, steps)
    sorted_data = np.sort(calc_data)
    n = len(sorted_data)
    
    for left_cut in cuts:
        for right_cut in cuts:
            if left_cut + right_cut >= 0.5: continue
            s = int(n * left_cut)
            e = int(n * (1 - right_cut))
            subset = sorted_data[s:e]
            
            if len(subset) > 20:
                # D√πng normaltest nhanh h∆°n shapiro v·ªõi data l·ªõn
                stat, p_val = stats.normaltest(subset)
                if p_val > best_p:
                    best_p = p_val
                    # Map l·∫°i percentile v√†o data g·ªëc
                    lower = np.percentile(data, left_cut * 100)
                    upper = np.percentile(data, (1 - right_cut) * 100)
                    best_range = (lower, upper)
    return best_range

# =========================================================
# üöÄ PH·∫¶N 2: HIGH-PERFORMANCE ENGINE (VECTORIZED)
# =========================================================

class PBRTQCEngine:
    def __init__(self, train_data, verify_data, trunc_range):
        # L∆∞u tr·ªØ d∆∞·ªõi d·∫°ng numpy array float64 ƒë·ªÉ t√≠nh to√°n nhanh nh·∫•t
        self.raw_train = np.array(train_data, dtype=np.float64)
        self.raw_verify = np.array(verify_data, dtype=np.float64)
        self.trunc_min, self.trunc_max = trunc_range
        
        # C·∫Øt g·ªçt d·ªØ li·ªáu (Vectorized filtering)
        self.train = self.raw_train[(self.raw_train >= self.trunc_min) & (self.raw_train <= self.trunc_max)]
        self.verify = self.raw_verify[(self.raw_verify >= self.trunc_min) & (self.raw_verify <= self.trunc_max)]

    def calculate_moving_metric(self, data, method, param):
        """T√≠nh to√°n MA b·∫±ng Pandas (ƒê√£ t·ªëi ∆∞u C-backend)"""
        # Chuy·ªÉn ƒë·ªïi nhanh sang Series ƒë·ªÉ d√πng h√†m c√≥ s·∫µn
        series = pd.Series(data)
        if method == 'SMA':
            # fillna(method='bfill') ƒë·ªÉ tr√°nh l·ªói NaN ·ªü ƒë·∫ßu
            return series.rolling(window=int(param)).mean().bfill().values
        elif method == 'EWMA':
            lam = 2 / (int(param) + 1)
            return series.ewm(alpha=lam, adjust=False).mean().values
        return data

    def determine_control_limits(self, method, param, target_fpr):
        ma_values = self.calculate_moving_metric(self.train, method, param)
        lower_percentile = (target_fpr / 2) * 100
        upper_percentile = 100 - (target_fpr / 2) * 100
        
        # np.percentile r·∫•t nhanh
        lcl = np.percentile(ma_values, lower_percentile)
        ucl = np.percentile(ma_values, upper_percentile)
        return lcl, ucl

    def run_simulation_vectorized(self, method, param, lcl, ucl, bias_pct, frequency=1, num_sims=50):
        """
        Phi√™n b·∫£n si√™u t·ªëc ƒë·ªô: S·ª≠ d·ª•ng NumPy Vectorization thay v√¨ v√≤ng l·∫∑p for
        """
        verify_data = self.verify
        n = len(verify_data)
        if n < 100: return {}, None

        # 1. T√≠nh Real FPR (Vectorized)
        ma_clean = self.calculate_moving_metric(verify_data, method, param)
        
        # T·∫°o m·∫£ng ch·ªâ s·ªë ƒë·ªÉ check frequency
        indices = np.arange(n)
        freq_mask = (indices % frequency == 0) # Ch·ªâ l·∫•y c√°c ƒëi·ªÉm ƒë√∫ng frequency
        
        # T√¨m c√°c ƒëi·ªÉm vi ph·∫°m
        violations = (ma_clean < lcl) | (ma_clean > ucl)
        
        # K·∫øt h·ª£p ƒëi·ªÅu ki·ªán: Vi ph·∫°m V√Ä ƒë√∫ng frequency
        valid_alarms = violations & freq_mask
        
        alarms_count = np.sum(valid_alarms)
        checks_count = np.sum(freq_mask)
        real_fpr = alarms_count / checks_count if checks_count > 0 else 0

        # 2. Simulation (Vectorized Search)
        detected_counts = []
        bias_factor = 1 + (bias_pct / 100.0)
        
        last_run_data = {}

        # Pre-calculate random start indices (Vectorized random)
        # Gi·ªõi h·∫°n ƒëi·ªÉm b·∫Øt ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o c√≤n √≠t nh·∫•t 50 m·∫´u ph√≠a sau
        start_indices = np.random.randint(20, max(21, n - 50), size=num_sims)

        for i, start_idx in enumerate(start_indices):
            # T·∫°o data m√¥ ph·ªèng
            # Copy m·∫£ng t·ªën √≠t th·ªùi gian h∆°n l√† t√≠nh to√°n l·∫°i t·ª´ ƒë·∫ßu
            sim_data = verify_data.copy()
            sim_data[start_idx:] *= bias_factor # Ph√©p nh√¢n t·∫°i ch·ªó (in-place) nhanh h∆°n
            
            # T√≠nh l·∫°i MA cho to√†n b·ªô chu·ªói (Pandas C-optimized r·∫•t nhanh, 40k d√≤ng ch·ªâ m·∫•t ~2ms)
            ma_sim = self.calculate_moving_metric(sim_data, method, param)
            
            # --- ƒêO·∫†N N√ÄY L√Ä QUAN TR·ªåNG NH·∫§T (T·ªêI ∆ØU H√ìA) ---
            # Thay v√¨ for loop t·ª´ng ph·∫ßn t·ª≠, ta d√πng mask
            
            # Ch·ªâ x√©t v√πng d·ªØ li·ªáu t·ª´ start_idx tr·ªü ƒëi
            region_of_interest = ma_sim[start_idx:]
            
            # 1. T√¨m ƒëi·ªÉm v∆∞·ª£t ng∆∞·ª°ng trong v√πng n√†y
            violation_mask = (region_of_interest < lcl) | (region_of_interest > ucl)
            
            # 2. T√¨m ƒëi·ªÉm ƒë√∫ng Frequency trong v√πng n√†y
            # C·∫ßn t√≠nh l·∫°i index to√†n c·ª•c cho v√πng n√†y
            global_indices_region = np.arange(start_idx, n)
            freq_mask_region = (global_indices_region % frequency == 0)
            
            # 3. K·∫øt h·ª£p ƒëi·ªÅu ki·ªán
            combined_mask = violation_mask & freq_mask_region
            
            # 4. T√¨m v·ªã tr√≠ True ƒë·∫ßu ti√™n (Argmax tr·∫£ v·ªÅ index ƒë·∫ßu ti√™n c·ªßa gi√° tr·ªã Max/True)
            if np.any(combined_mask):
                # np.argmax tr·∫£ v·ªÅ index t∆∞∆°ng ƒë·ªëi trong region
                relative_first_idx = np.argmax(combined_mask) 
                
                # S·ªë b·ªánh nh√¢n tr√¥i qua = index t∆∞∆°ng ƒë·ªëi + 1
                detected_counts.append(relative_first_idx + 1)
                
                # L∆∞u data l·∫ßn cu·ªëi ƒë·ªÉ v·∫Ω
                if i == num_sims - 1:
                    global_alarm_idx = start_idx + relative_first_idx
                    last_run_data = {
                        'ma_clean': ma_clean,
                        'ma_sim': ma_sim,
                        'start_idx': start_idx,
                        'alarm_idx': global_alarm_idx,
                        'lcl': lcl, 'ucl': ucl
                    }
            else:
                 # N·∫øu kh√¥ng t√¨m th·∫•y, v·∫´n l∆∞u data ƒë·ªÉ debug (kh√¥ng c√≥ alarm_idx)
                 if i == num_sims - 1:
                    last_run_data = {
                        'ma_clean': ma_clean,
                        'ma_sim': ma_sim,
                        'start_idx': start_idx,
                        'alarm_idx': None,
                        'lcl': lcl, 'ucl': ucl
                    }

        # T·ªïng h·ª£p ch·ªâ s·ªë
        if len(detected_counts) > 0:
            ped = len(detected_counts) / num_sims * 100
            anped = np.mean(detected_counts)
            mnped = np.median(detected_counts)
            nped95 = np.percentile(detected_counts, 95)
        else:
            ped = 0
            anped = mnped = nped95 = None

        return {
            "Real_FPR (%)": round(real_fpr * 100, 2),
            "Detection (%)": round(ped, 1),
            "ANPed": round(anped, 1) if anped else "N/A",
            "MNPed": round(mnped, 1) if mnped else "N/A",
            "95NPed": round(nped95, 1) if nped95 else "N/A"
        }, last_run_data

# =========================================================
# üöÄ PH·∫¶N 3: GIAO DI·ªÜN STREAMLIT
# =========================================================

st.set_page_config(layout="wide", page_title="PBRTQC High-Performance")

st.title("‚ö° PBRTQC Analyzer (High Performance Mode)")
st.markdown("H·ªá th·ªëng t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn (100k+ d√≤ng).")

with st.sidebar:
    st.header("1. Upload & C·∫•u h√¨nh")
    f_train = st.file_uploader("D·ªØ li·ªáu Training", type='xlsx')
    f_verify = st.file_uploader("D·ªØ li·ªáu Verify", type='xlsx')
    
    st.divider()
    bias_pct = st.number_input("Bias (%)", value=5.0, step=0.5)
    target_fpr = st.slider("Target FPR (%)", 0.1, 10.0, 2.0, 0.1) / 100
    model_type = st.selectbox("M√¥ h√¨nh", ["EWMA", "SMA"])
    
    # Th√™m t√πy ch·ªçn gi·∫£m s·ªë l·∫ßn m√¥ ph·ªèng n·∫øu m√°y y·∫øu
    num_sims = st.slider("S·ªë l·∫ßn m√¥ ph·ªèng (Simulations)", 10, 100, 50, 10, help="Gi·∫£m xu·ªëng n·∫øu th·∫•y ch·∫°y ch·∫≠m")

if f_train and f_verify:
    # ƒê·ªçc t√™n c·ªôt tr∆∞·ªõc (ƒê·ªÉ kh√¥ng cache sai c·ªôt)
    # Ph·∫ßn n√†y ƒë·ªçc nhanh header th√¥i
    df_preview = pd.read_excel(f_train, nrows=5)
    col_res = st.selectbox("Ch·ªçn c·ªôt K·∫øt qu·∫£:", df_preview.columns)
    
    # 1. LOAD DATA V·ªöI CACHE
    with st.spinner("ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn..."):
        data_train, data_verify = load_and_clean_data(f_train, f_verify, col_res)
        
    if data_train is not None:
        st.info(f"ƒê√£ t·∫£i: Training ({len(data_train):,} d√≤ng) - Verify ({len(data_verify):,} d√≤ng)")

        # 2. T√çNH TRUNCATION V·ªöI CACHE
        trunc_range = find_optimal_truncation(data_train)
        st.success(f"Truncation Range t·ªëi ∆∞u: [{trunc_range[0]:.2f} - {trunc_range[1]:.2f}]")
        
        # 3. KH·ªûI T·∫†O ENGINE
        engine = PBRTQCEngine(data_train, data_verify, trunc_range)

        # 4. C·∫§U H√åNH CASES
        st.header("C·∫•u h√¨nh Cases")
        cols = st.columns(3)
        cases = []
        for i, col in enumerate(cols):
            with col:
                bs = st.number_input(f"Block Size Case {i+1}", value=20*(i+1))
                freq = 1
                if model_type == "SMA":
                    freq = st.number_input(f"Freq Case {i+1}", value=1, min_value=1)
                cases.append({'bs': bs, 'freq': freq})

        # 5. CH·∫†Y SIMULATION
        if st.button("üöÄ CH·∫†Y ƒê√ÅNH GI√Å NGAY"):
            st.divider()
            results_table = []
            plot_data_list = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, case in enumerate(cases):
                status_text.text(f"ƒêang ch·∫°y Case {idx+1}/{len(cases)} v·ªõi {len(data_verify):,} d√≤ng d·ªØ li·ªáu...")
                
                # a. T√≠nh Limit
                lcl, ucl = engine.determine_control_limits(model_type, case['bs'], target_fpr)
                
                # b. Ch·∫°y Sim (D√πng h√†m Vectorized m·ªõi)
                metrics, plot_data = engine.run_simulation_vectorized(
                    model_type, case['bs'], lcl, ucl, bias_pct, 
                    frequency=case['freq'], num_sims=num_sims
                )
                
                row = {
                    "Case": f"Case {idx+1}",
                    "N": case['bs'],
                    "LCL": round(lcl, 2), "UCL": round(ucl, 2),
                    **metrics
                }
                results_table.append(row)
                plot_data_list.append({'name': f"Case {idx+1}", 'data': plot_data})
                
                progress_bar.progress((idx + 1) / len(cases))
            
            status_text.text("Ho√†n t·∫•t!")
            
            # HI·ªÇN TH·ªä K·∫æT QU·∫¢
            st.subheader("üìä K·∫øt qu·∫£")
            st.dataframe(pd.DataFrame(results_table).style.highlight_max(subset=['Detection (%)'], color='#d1ffbd'), use_container_width=True)
            
            # V·∫º BI·ªÇU ƒê·ªí
            st.divider()
            st.subheader("üìà Bi·ªÉu ƒë·ªì minh h·ªça")
            tabs = st.tabs([p['name'] for p in plot_data_list])
            
            for i, tab in enumerate(tabs):
                with tab:
                    d = plot_data_list[i]['data']
                    if d:
                        fig, ax = plt.subplots(figsize=(12, 4))
                        # V·∫Ω sample kho·∫£ng 1000 ƒëi·ªÉm quanh ƒëi·ªÉm l·ªói ƒë·ªÉ ƒë·ª° lag khi v·∫Ω
                        center = d['start_idx']
                        # V·∫Ω r·ªông ra 200 ƒëi·ªÉm tr∆∞·ªõc v√† 500 ƒëi·ªÉm sau l·ªói
                        s_plot = max(0, center - 200)
                        e_plot = min(len(d['ma_clean']), center + 500)
                        
                        x_axis = range(s_plot, e_plot)
                        
                        ax.plot(x_axis, d['ma_clean'][s_plot:e_plot], color='green', alpha=0.3, label='S·∫°ch')
                        ax.plot(x_axis, d['ma_sim'][s_plot:e_plot], color='orange', label='L·ªói')
                        ax.axhline(d['ucl'], color='red', ls='--'); ax.axhline(d['lcl'], color='red', ls='--')
                        ax.axvline(d['start_idx'], color='black', ls=':', label='B·∫Øt ƒë·∫ßu l·ªói')
                        
                        if d['alarm_idx'] and s_plot <= d['alarm_idx'] <= e_plot:
                            ax.scatter(d['alarm_idx'], d['ma_sim'][d['alarm_idx']], color='red', s=100, marker='*', zorder=5)
                        
                        ax.legend()
                        st.pyplot(fig)

    else:
        st.warning("Vui l√≤ng t·∫£i file l√™n.")
